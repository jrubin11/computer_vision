{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Spring 2019, Lab 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This program introduces the following concepts:\n",
    "\n",
    "*\t\ta) Reading a stream of images from a webcamera, and displaying the video (learned in lab 6)\n",
    "*\t\tb) Skin color detection (learned in lab 6)\n",
    "*\t\tc) Background differencing\n",
    "*\t\td) Visualizing motion history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from collections import deque\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(src, dst):\n",
    "    #mean squared error\n",
    "    err = np.sum(pow((src.astype(\"float\") - dst.astype(\"float\")),2))\n",
    "    err /= float(src.shape[0] * src.shape[1])\n",
    "\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skin color detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that detects whether a pixel belongs to the skin based on RGB values\n",
    "# src - the source color image\n",
    "# dst - the destination grayscale image where skin pixels are colored white and the rest are colored black\n",
    "def mySkinDetect(src):\n",
    "    # Surveys of skin color modeling and detection techniques:\n",
    "    # 1. Vezhnevets, Vladimir, Vassili Sazonov, and Alla Andreeva. \"A survey on pixel-based skin color detection techniques.\" Proc. Graphicon. Vol. 3. 2003.\n",
    "    # 2. Kakumanu, Praveen, Sokratis Makrogiannis, and Nikolaos Bourbakis. \"A survey of skin-color modeling and detection methods.\" Pattern recognition 40.3 (2007): 1106-1122.\n",
    "    dst = np.zeros((src.shape[0], src.shape[1], 1), dtype = \"uint8\")\n",
    "    for i in range(src.shape[0]):\n",
    "        for j in range(src.shape[1]):\n",
    "            #b,g,r = src[i,j]\n",
    "            b = int(src[i,j][0])\n",
    "            g = int(src[i,j][1])\n",
    "            r = int(src[i,j][2])\n",
    "            if(r>95 and g>40 and b>20 and max(r,g,b)-min(r,g,b)>15 and abs(r-g)>15 and r>g and r>b):\n",
    "                dst[i,j] = 255\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# frame-to-frame differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that does frame differencing between the current frame and the previous frame\n",
    "# prev - the previous color image\n",
    "# curr - the current color image\n",
    "# dst - the destination grayscale image where pixels are colored white if the corresponding pixel intensities in the current\n",
    "# and previous image are not the same\n",
    "def myFrameDifferencing(prev, curr):\n",
    "    # For more information on operation with arrays: \n",
    "    # http://docs.opencv.org/modules/core/doc/operations_on_arrays.html\n",
    "    dst = cv2.absdiff(prev, curr)\n",
    "    dst = cv2.cvtColor(dst, cv2.COLOR_BGR2GRAY)\n",
    "    _, dst = cv2.threshold(dst, 50, 255, cv2.THRESH_BINARY)\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# motion energy templates\n",
    "* example 1: the bottom row displays a cumulative binary motion energy image sequence corresponding to the frames above\n",
    "![title](mh1.png)\n",
    "* example 2: pixel intensity is a function of the motion history at that location, where brighter values correspond to more recent motion, three actions: sit-down, arms-raise, crouch-down\n",
    "![title](mh2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that accumulates the frame differences for a certain number of pairs of frames\n",
    "# mh - vector of frame difference images\n",
    "# dst - the destination grayscale image to store the accumulation of the frame difference images\n",
    "def myMotionEnergy(mh):\n",
    "    # the window of time is 3\n",
    "    mh0 = mh[0]\n",
    "    mh1 = mh[1]\n",
    "    mh2 = mh[2]\n",
    "    dst = np.zeros((mh0.shape[0], mh0.shape[1], 1), dtype = \"uint8\")\n",
    "    for i in range(mh0.shape[0]):\n",
    "        for j in range(mh0.shape[1]):\n",
    "            if mh0[i,j] == 255 or mh1[i,j] == 255 or mh2[i,j] == 255:\n",
    "                dst[i,j] = 255\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backgroundDifferencing(curr):\n",
    "    # For more information on operation with arrays: \n",
    "    # http://docs.opencv.org/modules/core/doc/operations_on_arrays.html\n",
    "    background = cv2.imread(\"background.png\")\n",
    "    dst = cv2.absdiff(curr, background)\n",
    "    dst = cv2.cvtColor(dst, cv2.COLOR_BGR2GRAY)\n",
    "    _, dst = cv2.threshold(dst, 50, 255, cv2.THRESH_BINARY)\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEdges(src):\n",
    "    \n",
    "    #get edges of skin detection image\n",
    "    left=src.shape[1]\n",
    "    right=0\n",
    "    down=0\n",
    "    up=src.shape[0]\n",
    "    for i in range(src.shape[0]):\n",
    "        for j in range(src.shape[1]):\n",
    "            if src[i][j]==255:\n",
    "                if j>right:\n",
    "                    right=j\n",
    "                elif j<left:\n",
    "                    left=j\n",
    "                elif i>down:\n",
    "                    down=i\n",
    "                elif i<up:\n",
    "                    up=i\n",
    "    return left,right,up,down\n",
    "#(0, 100, 0, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genstureSort(src,mSrc,gestures):\n",
    "    #convert to skin color\n",
    "    output=np.copy(src)\n",
    "    black=np.copy(mSrc)\n",
    "    _, black = cv2.threshold(black,0,0,cv2.THRESH_BINARY)\n",
    "    src=mySkinDetect(src)\n",
    "    #cv2.imshow(\"skinDetect\",cv2.resize(src,(300,200)))\n",
    "    gesture=None\n",
    "    minimum=os.sys.maxint\n",
    "    err=mse(black,mSrc)\n",
    "    #check for motion\n",
    "    if err>2500:\n",
    "        left,right,up,down=getEdges(mSrc)\n",
    "        cv2.putText(output,\"wave\", (5,90), cv2.FONT_HERSHEY_SIMPLEX, .35, (255,255,255))\n",
    "        cv2.rectangle(output,(left,up),(right,down), 255, 2)\n",
    "    else:\n",
    "        #otherwise template matching\n",
    "        for template in gestures:\n",
    "            result=cv2.matchTemplate(src,template.img,cv2.TM_SQDIFF)\n",
    "            minV, maxV, minL, maxL = cv2.minMaxLoc(result)\n",
    "            if minV<minimum:\n",
    "                minimum=minV\n",
    "                gesture=template         \n",
    "        #cv2.imshow(\"thegesture\",gesture.img)\n",
    "        cv2.rectangle(output,minL,(minL[0]+gesture.img.shape[1],minL[1]+gesture.img.shape[0]), 255, 2)\n",
    "        cv2.putText(output,gesture.gesture[:5], (5,90), cv2.FONT_HERSHEY_SIMPLEX, .35, (255,255,255))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gesture:\n",
    "    \n",
    "    #Gesture Class\n",
    "    def __init__(self,img):\n",
    "        self.gesture=img\n",
    "        if img[:5]==\"peace\":\n",
    "            self.img=mySkinDetect(cv2.resize(cv2.imread(img),(58,55)))\n",
    "        elif img[:5]==\"thumb\":\n",
    "            self.img=mySkinDetect(cv2.resize(cv2.imread(img),(50,58)))\n",
    "        else:\n",
    "            self.img=mySkinDetect(cv2.resize(cv2.imread(img),(63,85)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGestures():\n",
    "    i=0\n",
    "    gestures=[]\n",
    "    gNumber=0\n",
    "    \n",
    "    #read in gestures\n",
    "    while(True):\n",
    "        if gNumber==0:\n",
    "            if os.path.isfile(\"peace\"+str(i)+\".png\"):\n",
    "                gestures.append(Gesture(\"peace\"+str(i)+\".png\"))\n",
    "                i+=1\n",
    "                continue\n",
    "            else:\n",
    "                gNumber=1\n",
    "                i=0\n",
    "        elif gNumber==1:\n",
    "            if os.path.isfile(\"thumb\"+str(i)+\".png\"):\n",
    "                gestures.append(Gesture(\"thumb\"+str(i)+\".png\"))\n",
    "                i+=1\n",
    "                continue\n",
    "            else:\n",
    "                gNumber=2\n",
    "                i=0\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # a) Reading a stream of images from a webcamera, and displaying the video\n",
    "    # open the video camera no. 0\n",
    "    # for more information on reading and writing video: http://docs.opencv.org/modules/highgui/doc/reading_and_writing_images_and_video.html\n",
    "    \n",
    "    gestures=getGestures()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    #if not successful, exit program\n",
    "    if not cap.isOpened():\n",
    "        print(\"Cannot open the video cam\")\n",
    "        return -1\n",
    "\n",
    "    # read a new frame from video\n",
    "    success, prev_frame = cap.read()\n",
    "    \n",
    "    #if not successful, exit program\n",
    "    if not success:\n",
    "        print(\"Cannot read a frame from video stream\")\n",
    "        return -1\n",
    "    cv2.namedWindow(\"frame\", cv2.WINDOW_AUTOSIZE)\n",
    "    \n",
    "    prev_frame = cv2.resize(prev_frame,(150,100))\n",
    "    \n",
    "    fMH1 = np.zeros((prev_frame.shape[0], prev_frame.shape[1], 1), dtype = \"uint8\")\n",
    "    fMH2 = fMH1.copy()\n",
    "    fMH3 = fMH1.copy()\n",
    "    myMotionHistory = deque([fMH1, fMH2, fMH3]) \n",
    "    while(True):\n",
    "        #read a new frame from video\n",
    "        success, curr_frame = cap.read()\n",
    "        curr_frame = cv2.resize(curr_frame,(150,100))\n",
    "        if not success:\n",
    "            print(\"Cannot read a frame from video stream\")\n",
    "            break\n",
    "    \n",
    "        cv2.imshow('frame',curr_frame)\n",
    "\n",
    "        # c) Background differencing\n",
    "        frameDest = myFrameDifferencing(prev_frame, curr_frame)\n",
    "        myMotionHistory.popleft()\n",
    "        myMotionHistory.append(frameDest)\n",
    "        myMH = myMotionEnergy(myMotionHistory)\n",
    "        myMH = cv2.resize(myMH,(150,100))\n",
    "        #cv2.imshow('myMotionHistory',cv2.resize(myMH,(300,200)))\n",
    "        \n",
    "        # output with gesture detection\n",
    "        output=genstureSort(curr_frame,myMH,gestures)\n",
    "        output=cv2.resize(output,(150,100))\n",
    "        cv2.imshow(\"output\",output)\n",
    "        prev_frame = curr_frame\n",
    "        \n",
    "        # wait for 'q' key press. If 'q' key is pressed, break loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "(100, 150, 3)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "genstureSort() takes exactly 3 arguments (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-18c7c8dfdba1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mpeacesym\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"thumbsup.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeacesym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenstureSort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeacesym\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgestures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: genstureSort() takes exactly 3 arguments (2 given)"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "gestures=[]\n",
    "gNumber=0\n",
    "    \n",
    "while(True):\n",
    "    if gNumber==0:\n",
    "        if os.path.isfile(\"peace\"+str(i)+\".png\"):\n",
    "            gestures.append(Gesture(\"peace\"+str(i)+\".png\"))\n",
    "            i+=1\n",
    "            continue\n",
    "        else:\n",
    "            gNumber=1\n",
    "            i=0\n",
    "    elif gNumber==1:\n",
    "        if os.path.isfile(\"thumb\"+str(i)+\".png\"):\n",
    "            gestures.append(Gesture(\"thumb\"+str(i)+\".png\"))\n",
    "            i+=1\n",
    "            continue\n",
    "        else:\n",
    "            gNumber=2\n",
    "            i=0\n",
    "    else:\n",
    "        break\n",
    "    print(i)\n",
    "        \n",
    "peacesym=cv2.resize(cv2.imread(\"thumbsup.png\"),(150,100))\n",
    "print(peacesym.shape)\n",
    "out=genstureSort(peacesym,gestures)\n",
    "cv2.imshow(\"output\",out)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "    \n",
    "#if not successful, exit program\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open the video cam\")\n",
    "\n",
    "    # read a new frame from video\n",
    "success, prev_frame = cap.read()\n",
    "cv2.imshow(\"backstart\",prev_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genstureSort(src,mSrc,gestures):\n",
    "    output=np.copy(src)\n",
    "    black=np.copy(mSrc)\n",
    "    _, black = cv2.threshold(black,0,0,cv2.THRESH_BINARY)\n",
    "    src=mySkinDetect(src)\n",
    "    gesture=None\n",
    "    minimum=os.sys.maxint\n",
    "    err=mse(black,mSrc)\n",
    "    if err>3000:\n",
    "        cv2.putText(output,\"wave\", (5,90), cv2.FONT_HERSHEY_SIMPLEX, .35, (255,255,255))\n",
    "    else:\n",
    "        for template in gestures:\n",
    "            result=cv2.matchTemplate(src,template.img,cv2.TM_SQDIFF)\n",
    "            minV, maxV, minL, maxL = cv2.minMaxLoc(result)\n",
    "            if minV<minimum:\n",
    "                minimum=minV\n",
    "                gesture=template         \n",
    "        #cv2.imshow(\"thegesture\",gesture.img)\n",
    "        cv2.rectangle(output,minL,(minL[0]+gesture.img.shape[1],minL[1]+gesture.img.shape[0]), 255, 2)\n",
    "        cv2.putText(output,gesture.gesture[:5], (5,90), cv2.FONT_HERSHEY_SIMPLEX, .35, (255,255,255))\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
